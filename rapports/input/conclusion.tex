\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion} 

Dans ce rapport, nous avons commencé par étudier \textbf{Lucene} : nous avons présenté le projet avant d'entrer dans les détails du fonctionnement de la bibliothèque logicielle. Nous avons ainsi expliqué la notion d'indexation et d'interrogation, et nous avons pu voir les spécificités de \textbf{Lucene} : utilisation d'un analyseur qui décompose un texte d'entrée en termes, qui seront plus tard filtrés. Nous avons ainsi pu comprendre ce qui fait le succès de la bibliothèque : ces deux abstractions (découpage en termes et filtrage) permettent une grande modularité, puisqu'il est possible de créer ses propres \textit{tokenizers} et filtres.

Après avoir étudié le fonctionnement de \textbf{Lucene}, nous avons présenté le support de notre étude : la collection \textbf{CISI}, et les différents indicateurs que nous avons décidé de tirer de l'étude comparative. Nous avons ainsi pu voir que la collection \textbf{CISI} contient quatre fichiers importants : les articles à indexer, les requêtes à soumettre au moteur de recherche testé, les résultats théoriques et une liste de mots vides (spécialement préparée pour cette collection d'articles). Les critères que nous avons décidé de retenir se découpent quant à eux en 3 catégories : coût en temps d'exécution (pour l'indexation comme pour l'interrogation), coût en mémoire (taille de l'index), qualité des résultats (rappel et précision).

Nous avons grâce à cela pu présenté le programme réalisé. Nous avons expliqué comment sont implémentées les différentes étapes nécessaires au fonctionnement de \textbf{Lucene} telles que définies dans la première partie. Nous avons aussi indiqué où et comment étaient calculés les critères retenus, présentés dans la partie 2, en prenant en compte les contraintes des machines utilisées (avec notamment le non déterminisme du temps d'exécution, qui a donc besoin d'être précautionneusement calculé pour pouvoir être comparé).

Enfin, nous avons présenté le résultat de nos analyses, à la fois sur les \textit{tokenizers} et les filtres. Nous avons pu constater une nette amélioration des résultats grâce aux filtres sur les mots vides d'une part, et le filtre sur les synonymes d'autre part. Ce dernier est néanmoins très coûteux en termes de temps d'exécution. On remarque aussi les très bons résultats du \texttt{SnowBallFilter}.

Néanmoins, la précision reste faible dans tous les cas (elle ne franchit jamais les 4\%). Bien qu'obtenir les meilleurs résultats possibles n'était pas l'objectif de cette étude, on peut tout de même essayer d'en comprendre les causes. Les moteurs de recherche (y compris les plus connus) ont en général de très mauvaises précisions. En effet, en tapant un seul mot commun sur les moteurs de recherche grands publics, on constate très souvent des millions et des millions de résultats, alors qu'une dizaine de documents seulement répondent à notre requête. Cela est largement compensé par le fait que les résultats sont triés, de manière à s'assurer que les documents les plus pertinents apparaissent dès les premiers résultats. La collection \texttt{CISI} ne contenant aucune information concernant l'ordre dans lequel les résultats devraient théoriquement être renvoyés, nous n'avons pas pu apprécier ce critère.